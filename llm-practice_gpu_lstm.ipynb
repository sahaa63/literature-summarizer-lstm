{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "PyTorch version: 2.0.1+cu118\n",
      "CUDA version: 11.8\n",
      "Test tensor on GPU successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "torch.cuda.empty_cache()\n",
    "test_tensor = torch.rand(1000, 1000).cuda()\n",
    "print(\"Test tensor on GPU successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to literature_corpus.txt\n",
      "Corpus length: 1984344 characters\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "books = {\n",
    "    \"Pride and Prejudice\": \"https://www.gutenberg.org/files/1342/1342-0.txt\",\n",
    "    \"Moby Dick\": \"https://www.gutenberg.org/files/2701/2701-0.txt\"\n",
    "}\n",
    "\n",
    "corpus = \"\"\n",
    "for title, url in books.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        corpus += response.text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {title}: {e}\")\n",
    "\n",
    "with open(\"literature_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(corpus)\n",
    "\n",
    "print(\"Data saved to literature_corpus.txt\")\n",
    "print(f\"Corpus length: {len(corpus)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 3876\n",
      "Sample chunk: *** START OF THE PROJECT GUTENBERG EBOOK 1342 ***\n",
      "\n",
      "                            [Illustration:\n",
      "\n",
      "\n",
      "\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "try:\n",
    "    with open(\"literature_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: literature_corpus.txt not found. Run Cell 2 first.\")\n",
    "    exit()\n",
    "\n",
    "def chunk_text(text, chunk_size=512):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "text_chunks = chunk_text(text)\n",
    "dataset = Dataset.from_dict({\"text\": text_chunks})\n",
    "\n",
    "print(f\"Number of chunks: {len(text_chunks)}\")\n",
    "print(\"Sample chunk:\", text_chunks[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully\n",
      "Dataset info: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 3876\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3876/3876 [00:01<00:00, 3271.16 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed successfully\n",
      "Tokenized dataset ready: Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 3876\n",
      "})\n",
      "Sample input IDs: [8162, 33303, 3963, 3336, 21965, 23680, 402, 3843, 1677, 13246]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")  # Using distilgpt2 tokenizer for simplicity\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Tokenizer loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load tokenizer: {e}\")\n",
    "    exit()\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    try:\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error in tokenize_function: {e}\")\n",
    "        raise\n",
    "\n",
    "if 'dataset' not in globals():\n",
    "    print(\"Error: 'dataset' not defined. Run Cell 3 first.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Dataset info:\", dataset)\n",
    "\n",
    "try:\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    print(\"Tokenization completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Tokenization failed: {e}\")\n",
    "    raise\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "print(\"Tokenized dataset ready:\", tokenized_dataset)\n",
    "print(\"Sample input IDs:\", tokenized_dataset[0][\"input_ids\"][:10].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "PyTorch version: 2.0.1+cu118\n",
      "CUDA version: 11.8\n",
      "Test tensor on GPU successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\sahaa\\anaconda3\\envs\\llm_env\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\sahaa\\anaconda3\\envs\\llm_env\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\sahaa\\anaconda3\\envs\\llm_env\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\sahaa\\anaconda3\\envs\\llm_env\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\sahaa\\anaconda3\\envs\\llm_env\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\sahaa\\AppData\\Local\\Temp\\ipykernel_28244\\1929566708.py\", line 7, in <module>\n",
      "    test_tensor = torch.rand(1000, 1000).cuda()\n",
      "C:\\Users\\sahaa\\AppData\\Local\\Temp\\ipykernel_28244\\1929566708.py:7: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  test_tensor = torch.rand(1000, 1000).cuda()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "torch.cuda.empty_cache()\n",
    "test_tensor = torch.rand(1000, 1000).cuda()\n",
    "print(\"Test tensor on GPU successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to literature_corpus.txt\n",
      "Corpus length: 1984344 characters\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "books = {\n",
    "    \"Pride and Prejudice\": \"https://www.gutenberg.org/files/1342/1342-0.txt\",\n",
    "    \"Moby Dick\": \"https://www.gutenberg.org/files/2701/2701-0.txt\"\n",
    "}\n",
    "\n",
    "corpus = \"\"\n",
    "for title, url in books.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        corpus += response.text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {title}: {e}\")\n",
    "\n",
    "with open(\"literature_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(corpus)\n",
    "\n",
    "print(\"Data saved to literature_corpus.txt\")\n",
    "print(f\"Corpus length: {len(corpus)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 3876\n",
      "Sample chunk: *** START OF THE PROJECT GUTENBERG EBOOK 1342 ***\n",
      "\n",
      "                            [Illustration:\n",
      "\n",
      "\n",
      "\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "try:\n",
    "    with open(\"literature_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: literature_corpus.txt not found. Run Cell 2 first.\")\n",
    "    exit()\n",
    "\n",
    "def chunk_text(text, chunk_size=512):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "text_chunks = chunk_text(text)\n",
    "dataset = Dataset.from_dict({\"text\": text_chunks})\n",
    "\n",
    "print(f\"Number of chunks: {len(text_chunks)}\")\n",
    "print(\"Sample chunk:\", text_chunks[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahaa\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully\n",
      "Dataset info: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 3876\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3876/3876 [00:01<00:00, 3631.76 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed successfully\n",
      "Tokenized dataset ready: Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 3876\n",
      "})\n",
      "Sample input IDs: [8162, 33303, 3963, 3336, 21965, 23680, 402, 3843, 1677, 13246]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")  # Using distilgpt2 tokenizer for simplicity\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Tokenizer loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load tokenizer: {e}\")\n",
    "    exit()\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    try:\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error in tokenize_function: {e}\")\n",
    "        raise\n",
    "\n",
    "if 'dataset' not in globals():\n",
    "    print(\"Error: 'dataset' not defined. Run Cell 3 first.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Dataset info:\", dataset)\n",
    "\n",
    "try:\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    print(\"Tokenization completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Tokenization failed: {e}\")\n",
    "    raise\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "print(\"Tokenized dataset ready:\", tokenized_dataset)\n",
    "print(\"Sample input IDs:\", tokenized_dataset[0][\"input_ids\"][:10].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized on cuda\n",
      "Starting training...\n",
      "Epoch 1 complete, Loss: 1.7979\n",
      "LSTM training complete and saved to lstm_summarizer.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define LSTM model\n",
    "class LSTMSummarizer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "\n",
    "# Verify tokenized_dataset\n",
    "if 'tokenized_dataset' not in globals():\n",
    "    print(\"Error: 'tokenized_dataset' not defined. Run Cell 4 first.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = LSTMSummarizer(vocab_size).to(device)\n",
    "print(f\"Model initialized on {device}\")\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "try:\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(1):  # 1 epoch for speed\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[\"input_ids\"].to(device)\n",
    "            targets = inputs[:, 1:].contiguous()  # Shifted input as target\n",
    "            inputs = inputs[:, :-1]  # Remove last token for input\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1} complete, Loss: {loss.item():.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {e}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    exit()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"lstm_summarizer.pth\")\n",
    "print(\"LSTM training complete and saved to lstm_summarizer.pth\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully on cuda\n",
      "Generated response: Mr. Darcy, a wealthy but aloof gentleman, initially clashes with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define LSTM model (must match Cell 5)\n",
    "class LSTMSummarizer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out, hidden\n",
    "\n",
    "# Load tokenizer and model\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = LSTMSummarizer(vocab_size).to(device)\n",
    "    model.load_state_dict(torch.load(\"lstm_summarizer.pth\"))\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully on\", device)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model/tokenizer: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Test generation\n",
    "prompt = \"Mr. Darcy, a wealthy but aloof gentleman, initially clashes with\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        hidden = None\n",
    "        generated_ids = inputs\n",
    "        for _ in range(50):  # Generate 50 tokens\n",
    "            outputs, hidden = model(generated_ids, hidden)\n",
    "            next_token = torch.argmax(outputs[:, -1, :], dim=-1).unsqueeze(0)\n",
    "            generated_ids = torch.cat((generated_ids, next_token), dim=1)\n",
    "        response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(\"Generated response:\", response)\n",
    "except Exception as e:\n",
    "    print(f\"Generation failed: {e}\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
